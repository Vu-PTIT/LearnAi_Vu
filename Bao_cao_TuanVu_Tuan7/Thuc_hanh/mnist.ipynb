{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3cdfdad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9.91M/9.91M [00:10<00:00, 919kB/s] \n",
      "100%|██████████| 28.9k/28.9k [00:00<00:00, 61.4kB/s]\n",
      "100%|██████████| 1.65M/1.65M [00:02<00:00, 569kB/s]\n",
      "100%|██████████| 4.54k/4.54k [00:00<00:00, 1.52MB/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "bs = 100\n",
    "# MNIST Dataset\n",
    "train_dataset = datasets.MNIST(root='/kaggle/working/mnist-data', train=True, transform=transforms.ToTensor(), download=True)\n",
    "test_dataset = datasets.MNIST(root='/kaggle/working/mnist-data', train=False, transform=transforms.ToTensor(), download=True)\n",
    "\n",
    "# Data Loader (Input Pipeline)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=bs, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=bs, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "308e839b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, x_dim, h_dim1, h_dim2, z_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        # encoder part\n",
    "        self.fc1 = nn.Linear(x_dim, h_dim1)\n",
    "        self.fc2 = nn.Linear(h_dim1, h_dim2)\n",
    "        self.fc31 = nn.Linear(h_dim2, z_dim)\n",
    "        self.fc32 = nn.Linear(h_dim2, z_dim)\n",
    "        # decoder part\n",
    "        self.fc4 = nn.Linear(z_dim, h_dim2)\n",
    "        self.fc5 = nn.Linear(h_dim2, h_dim1)\n",
    "        self.fc6 = nn.Linear(h_dim1, x_dim)\n",
    "        \n",
    "    def encoder(self, x):\n",
    "        h = F.relu(self.fc1(x))\n",
    "        h = F.relu(self.fc2(h))\n",
    "        return self.fc31(h), self.fc32(h) # mu, log_var\n",
    "    \n",
    "    def sampling(self, mu, log_var):\n",
    "        std = torch.exp(0.5*log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps.mul(std).add_(mu) # return z sample\n",
    "        \n",
    "    def decoder(self, z):\n",
    "        h = F.relu(self.fc4(z))\n",
    "        h = F.relu(self.fc5(h))\n",
    "        return F.sigmoid(self.fc6(h)) \n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, log_var = self.encoder(x.view(-1, 784))\n",
    "        z = self.sampling(mu, log_var)\n",
    "        return self.decoder(z), mu, log_var\n",
    "\n",
    "# build model\n",
    "vae = VAE(x_dim=784, h_dim1= 512, h_dim2=256, z_dim=2)\n",
    "if torch.cuda.is_available():\n",
    "    vae.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10e6a150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 1, 28, 28])\n",
      "torch.Size([100, 16, 14, 14])\n",
      "torch.Size([100, 32, 7, 7])\n",
      "torch.Size([100, 2])\n",
      "torch.Size([100, 32, 7, 7])\n",
      "torch.Size([100, 16, 14, 14])\n",
      "torch.Size([100, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "for i,(data,_) in enumerate(train_loader):\n",
    "    print(data.shape)\n",
    "\n",
    "    h = nn.Conv2d(1, 16, 3, 2, 1)(data)\n",
    "    print(h.shape)\n",
    "    h = nn.Conv2d(16, 32, 3, 2, 1)(h)\n",
    "    print(h.shape)\n",
    "    h = nn.Linear(7*7*32, 2)(h.view(-1, 7*7*32))\n",
    "    print(h.shape)\n",
    "\n",
    "    h = nn.Linear(2, 7*7*32)(h).view(-1, 32, 7, 7)\n",
    "    print(h.shape)\n",
    "    h = nn.ConvTranspose2d(32, 16, 3, 2, output_padding=1, padding=1)(h)\n",
    "    print(h.shape)\n",
    "    h = nn.ConvTranspose2d(16, 1, 3, 2, output_padding=1, padding=1)(h)\n",
    "    print(h.shape)\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9037f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE_CNN(nn.Module):\n",
    "    def __init__(self, x_dim, h_dim1, h_dim2, z_dim):\n",
    "        super(VAE_CNN, self).__init__()\n",
    "        \n",
    "        # encoder part\n",
    "        # self.fc1 = nn.Linear(x_dim, h_dim1)\n",
    "        # self.fc2 = nn.Linear(h_dim1, h_dim2)\n",
    "        # self.fc31 = nn.Linear(h_dim2, z_dim)\n",
    "        # self.fc32 = nn.Linear(h_dim2, z_dim)\n",
    "        self.cnn1 = nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1)\n",
    "        self.cnn2 = nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1)\n",
    "        self.fc3 = nn.Linear(32*7*7, h_dim1)\n",
    "        self.fc4 = nn.Linear(h_dim1, h_dim2)\n",
    "        self.fc51 = nn.Linear(h_dim2, z_dim)\n",
    "        self.fc52 = nn.Linear(h_dim2, z_dim)\n",
    " # decoder part\n",
    "        # self.fc4 = nn.Linear(z_dim, h_dim2)\n",
    "        # self.fc5 = nn.Linear(h_dim2, h_dim1)\n",
    "        # self.fc6 = nn.Linear(h_dim1, x_dim)\n",
    "        self.fc6 = nn.Linear(z_dim, h_dim2)\n",
    "        self.fc7 = nn.Linear(h_dim2, h_dim1)\n",
    "        self.fc8 = nn.Linear(h_dim1, 32*7*7)\n",
    "        self.tcnn9 = nn.ConvTranspose2d(32, 16, kernel_size=3, stride=2, output_padding=1, padding=1)\n",
    "        self.tcnn10 = nn.ConvTranspose2d(16, 1, kernel_size=3, stride=2, output_padding=1, padding=1)\n",
    "        \n",
    "    def encoder(self, x):\n",
    "        # h = F.relu(self.fc1(x))\n",
    "        # h = F.relu(self.fc2(h))\n",
    "        # return self.fc31(h), self.fc32(h) # mu, log_var\n",
    "        h = F.relu(self.cnn1(x))\n",
    "        h = F.relu(self.cnn2(h))\n",
    "        h = F.relu(self.fc3(h.view(-1, 32*7*7)))\n",
    "        h = F.relu(self.fc4(h))\n",
    "        return self.fc51(h), self.fc52(h)\n",
    "\n",
    "    def sampling(self, mu, log_var):\n",
    "        std = torch.exp(0.5*log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps.mul(std).add_(mu) # return z sample\n",
    "        \n",
    "    def decoder(self, z):\n",
    "        # h = F.relu(self.fc4(z))\n",
    "        # h = F.relu(self.fc5(h))\n",
    "        # return F.sigmoid(self.fc6(h)) \n",
    "        h = F.relu(self.fc6(z))\n",
    "        h = F.relu(self.fc7(h))\n",
    "        h = F.relu(self.fc8(h)).view(-1, 32, 7, 7)\n",
    "        h = F.relu(self.tcnn9(h))\n",
    "        return torch.sigmoid(self.tcnn10(h))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # mu, log_var = self.encoder(x.view(-1, 784))\n",
    "        mu, log_var = self.encoder(x)\n",
    "        z = self.sampling(mu, log_var)\n",
    "        return self.decoder(z), mu, log_var\n",
    "\n",
    "# build model\n",
    "vae = VAE_CNN(x_dim=784, h_dim1= 512, h_dim2=256, z_dim=2)\n",
    "if torch.cuda.is_available():\n",
    "    vae.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f345e49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE_CNN_Singular(nn.Module):\n",
    "    def __init__(self, x_dim, h_dim1, h_dim2, z_dim):\n",
    "        super(VAE_CNN_Singular, self).__init__()\n",
    "        \n",
    "        # encoder part\n",
    "        self.cnn1 = nn.Conv2d(1, 32, kernel_size=3, stride=2, padding=1)\n",
    "        #self.cnn2 = nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1)\n",
    "        self.fc3 = nn.Linear(32*14*14, h_dim1)\n",
    "        self.fc4 = nn.Linear(h_dim1, h_dim2)\n",
    "        self.fc51 = nn.Linear(h_dim2, z_dim)\n",
    "        self.fc52 = nn.Linear(h_dim2, z_dim)\n",
    "\n",
    "        # decoder part\n",
    "        self.fc6 = nn.Linear(z_dim, h_dim2)\n",
    "        self.fc7 = nn.Linear(h_dim2, h_dim1)\n",
    "        self.fc8 = nn.Linear(h_dim1, 32*14*14)\n",
    "        #self.tcnn9 = nn.ConvTranspose2d(32, 16, kernel_size=3, stride=2, output_padding=1, padding=1)\n",
    "        self.tcnn10 = nn.ConvTranspose2d(32, 1, kernel_size=3, stride=2, output_padding=1, padding=1)\n",
    "        \n",
    "    def encoder(self, x):\n",
    "        h = F.relu(self.cnn1(x))\n",
    "        h = F.relu(self.cnn2(h))\n",
    "        h = F.relu(self.fc3(h.view(-1, 32*7*7)))\n",
    "        h = F.relu(self.fc4(h))\n",
    "        return self.fc51(h), self.fc52(h)\n",
    "\n",
    "    def sampling(self, mu, log_var):\n",
    "        std = torch.exp(0.5*log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps.mul(std).add_(mu) # return z sample\n",
    "        \n",
    "    def decoder(self, z):\n",
    "        h = F.relu(self.fc6(z))\n",
    "        h = F.relu(self.fc7(h))\n",
    "        h = F.relu(self.fc8(h)).view(-1, 32, 7, 7)\n",
    "        h = F.relu(self.tcnn9(h))\n",
    "        return torch.sigmoid(self.tcnn10(h))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, log_var = self.encoder(x)\n",
    "        z = self.sampling(mu, log_var)\n",
    "        return self.decoder(z), mu, log_var\n",
    "\n",
    "# build model\n",
    "vae_singular = VAE_CNN_Singular(x_dim=784, h_dim1= 512, h_dim2=256, z_dim=2)\n",
    "if torch.cuda.is_available():\n",
    "    vae.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ff34fa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VAE_CNN(\n",
       "  (cnn1): Conv2d(1, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "  (cnn2): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "  (fc3): Linear(in_features=1568, out_features=512, bias=True)\n",
       "  (fc4): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (fc51): Linear(in_features=256, out_features=2, bias=True)\n",
       "  (fc52): Linear(in_features=256, out_features=2, bias=True)\n",
       "  (fc6): Linear(in_features=2, out_features=256, bias=True)\n",
       "  (fc7): Linear(in_features=256, out_features=512, bias=True)\n",
       "  (fc8): Linear(in_features=512, out_features=1568, bias=True)\n",
       "  (tcnn9): ConvTranspose2d(32, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "  (tcnn10): ConvTranspose2d(16, 1, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0ac4571",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VAE_CNN_Singular(\n",
       "  (cnn1): Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "  (fc3): Linear(in_features=6272, out_features=512, bias=True)\n",
       "  (fc4): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (fc51): Linear(in_features=256, out_features=2, bias=True)\n",
       "  (fc52): Linear(in_features=256, out_features=2, bias=True)\n",
       "  (fc6): Linear(in_features=2, out_features=256, bias=True)\n",
       "  (fc7): Linear(in_features=256, out_features=512, bias=True)\n",
       "  (fc8): Linear(in_features=512, out_features=6272, bias=True)\n",
       "  (tcnn10): ConvTranspose2d(32, 1, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae_singular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d1e004b",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(vae.parameters())\n",
    "# return reconstruction error + KL divergence losses\n",
    "def loss_function(recon_x, x, mu, log_var):\n",
    "    # BCE = F.binary_cross_entropy(recon_x, x.view(-1, 784), reduction='sum')\n",
    "    BCE = F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "    return BCE + KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e3ba9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    vae.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = data.cpu()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        recon_batch, mu, log_var = vae(data)\n",
    "        loss = loss_function(recon_batch, data, mu, log_var)\n",
    "        \n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item() / len(data)))\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(epoch, train_loss / len(train_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a8d268",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    vae.eval()\n",
    "    test_loss= 0\n",
    "    with torch.no_grad():\n",
    "        for data, _ in test_loader:\n",
    "            data = data.cpu()\n",
    "            recon, mu, log_var = vae(data)\n",
    "            \n",
    "            # sum up batch loss\n",
    "            test_loss += loss_function(recon, data, mu, log_var).item()\n",
    "        \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('====> Test set loss: {:.4f}'.format(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5f38ea27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 583.328242\n",
      "Train Epoch: 1 [10000/60000 (17%)]\tLoss: 198.451758\n",
      "Train Epoch: 1 [20000/60000 (33%)]\tLoss: 194.958887\n",
      "Train Epoch: 1 [30000/60000 (50%)]\tLoss: 179.611973\n",
      "Train Epoch: 1 [40000/60000 (67%)]\tLoss: 180.459688\n",
      "Train Epoch: 1 [50000/60000 (83%)]\tLoss: 187.943379\n",
      "====> Epoch: 1 Average loss: 198.6540\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m51\u001b[39m):\n\u001b[0;32m      2\u001b[0m     train(epoch)\n\u001b[1;32m----> 3\u001b[0m     \u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[10], line 6\u001b[0m, in \u001b[0;36mtest\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m data, _ \u001b[38;5;129;01min\u001b[39;00m test_loader:\n\u001b[1;32m----> 6\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m         recon, mu, log_var \u001b[38;5;241m=\u001b[39m vae(data)\n\u001b[0;32m      9\u001b[0m         \u001b[38;5;66;03m# sum up batch loss\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\books\\lib\\site-packages\\torch\\cuda\\__init__.py:363\u001b[0m, in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    358\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    359\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    360\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    361\u001b[0m     )\n\u001b[0;32m    362\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 363\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    364\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    365\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[0;32m    366\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    367\u001b[0m     )\n",
      "\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 51):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90d97cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    z = torch.randn(64, 2).cpu()\n",
    "    for i in range(2):\n",
    "        z[:, i] = torch.linspace(-5, 0, 64)\n",
    "\n",
    "    sample = vae.decoder(z).cpu()\n",
    "    \n",
    "    save_image(sample.view(64, 1, 28, 28), '/kaggle/working/sample1_' + '.png')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "137105e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[2.2655e-27, 9.5233e-36, 1.4196e-33,  ..., 2.0901e-20,\n",
      "           2.0386e-20, 1.6351e-18],\n",
      "          [4.6950e-31, 0.0000e+00, 4.3078e-39,  ..., 9.6937e-23,\n",
      "           1.8220e-19, 3.3764e-15],\n",
      "          [1.3945e-29, 0.0000e+00, 0.0000e+00,  ..., 6.1022e-21,\n",
      "           1.8666e-18, 1.3398e-13],\n",
      "          ...,\n",
      "          [8.3594e-30, 4.7067e-29, 6.9267e-24,  ..., 4.2606e-23,\n",
      "           1.8737e-23, 1.0894e-19],\n",
      "          [2.0808e-22, 4.8173e-31, 2.5587e-24,  ..., 1.3288e-27,\n",
      "           1.9216e-21, 8.1219e-14],\n",
      "          [2.6182e-19, 4.0590e-27, 1.8386e-17,  ..., 1.8349e-21,\n",
      "           1.3749e-14, 6.2008e-14]]],\n",
      "\n",
      "\n",
      "        [[[5.5300e-27, 3.0668e-35, 4.2671e-33,  ..., 4.0008e-20,\n",
      "           3.9098e-20, 2.9518e-18],\n",
      "          [1.2991e-30, 0.0000e+00, 1.5554e-38,  ..., 1.9833e-22,\n",
      "           3.3731e-19, 5.3920e-15],\n",
      "          [3.6732e-29, 0.0000e+00, 0.0000e+00,  ..., 1.1737e-20,\n",
      "           3.3276e-18, 2.0368e-13],\n",
      "          ...,\n",
      "          [2.1619e-29, 1.1834e-28, 1.4755e-23,  ..., 9.0693e-23,\n",
      "           3.9937e-23, 2.0509e-19],\n",
      "          [4.2375e-22, 1.2984e-30, 5.5272e-24,  ..., 3.2554e-27,\n",
      "           3.8246e-21, 1.2561e-13],\n",
      "          [4.8057e-19, 9.6064e-27, 3.1801e-17,  ..., 3.6868e-21,\n",
      "           2.1816e-14, 9.6292e-14]]],\n",
      "\n",
      "\n",
      "        [[[1.3499e-26, 9.8762e-35, 1.2827e-32,  ..., 7.6580e-20,\n",
      "           7.4986e-20, 5.3284e-18],\n",
      "          [3.5944e-30, 8.6083e-39, 5.6161e-38,  ..., 4.0577e-22,\n",
      "           6.2444e-19, 8.6110e-15],\n",
      "          [9.6755e-29, 0.0000e+00, 0.0000e+00,  ..., 2.2573e-20,\n",
      "           5.9323e-18, 3.0963e-13],\n",
      "          ...,\n",
      "          [5.5909e-29, 2.9756e-28, 3.1432e-23,  ..., 1.9305e-22,\n",
      "           8.5126e-23, 3.8612e-19],\n",
      "          [8.6300e-22, 3.4996e-30, 1.1939e-23,  ..., 7.9750e-27,\n",
      "           7.6122e-21, 1.9426e-13],\n",
      "          [8.8207e-19, 2.2736e-26, 5.5002e-17,  ..., 7.4077e-21,\n",
      "           3.4615e-14, 1.4953e-13]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[7.6897e-07, 1.0273e-08, 3.5390e-08,  ..., 1.8152e-05,\n",
      "           1.8537e-05, 6.3714e-05],\n",
      "          [1.6608e-07, 1.3236e-09, 3.0104e-09,  ..., 1.2691e-06,\n",
      "           1.7450e-05, 1.2710e-04],\n",
      "          [3.6648e-07, 9.5341e-11, 1.6042e-09,  ..., 5.1105e-06,\n",
      "           1.8510e-05, 2.6206e-04],\n",
      "          ...,\n",
      "          [8.0684e-08, 1.9868e-07, 1.5547e-06,  ..., 1.3319e-05,\n",
      "           3.9753e-06, 3.6590e-05],\n",
      "          [5.2959e-06, 4.4116e-08, 6.3992e-07,  ..., 3.8598e-07,\n",
      "           7.8814e-06, 6.2533e-04],\n",
      "          [3.3225e-05, 7.2363e-07, 7.2498e-05,  ..., 2.0930e-05,\n",
      "           4.3156e-04, 7.3521e-04]]],\n",
      "\n",
      "\n",
      "        [[[9.8865e-07, 1.4999e-08, 4.9057e-08,  ..., 2.3213e-05,\n",
      "           2.3180e-05, 7.6827e-05],\n",
      "          [2.2814e-07, 2.0186e-09, 4.5391e-09,  ..., 1.6179e-06,\n",
      "           2.1388e-05, 1.5389e-04],\n",
      "          [4.9275e-07, 1.6209e-10, 2.7374e-09,  ..., 6.5613e-06,\n",
      "           2.1769e-05, 2.9472e-04],\n",
      "          ...,\n",
      "          [1.1984e-07, 3.1860e-07, 2.1335e-06,  ..., 1.6092e-05,\n",
      "           4.8829e-06, 4.3550e-05],\n",
      "          [6.9601e-06, 6.6072e-08, 7.9764e-07,  ..., 4.8499e-07,\n",
      "           9.2412e-06, 6.8952e-04],\n",
      "          [4.3370e-05, 1.1208e-06, 8.8294e-05,  ..., 2.5122e-05,\n",
      "           4.8193e-04, 8.2797e-04]]],\n",
      "\n",
      "\n",
      "        [[[1.1574e-06, 1.9901e-08, 6.0897e-08,  ..., 2.8545e-05,\n",
      "           2.7250e-05, 8.6740e-05],\n",
      "          [2.8518e-07, 2.7580e-09, 6.0372e-09,  ..., 1.9865e-06,\n",
      "           2.5094e-05, 1.8187e-04],\n",
      "          [6.0164e-07, 2.4452e-10, 4.3365e-09,  ..., 8.1256e-06,\n",
      "           2.4677e-05, 3.2452e-04],\n",
      "          ...,\n",
      "          [1.6180e-07, 4.9176e-07, 2.8282e-06,  ..., 1.7465e-05,\n",
      "           5.4938e-06, 4.8320e-05],\n",
      "          [8.4327e-06, 9.1643e-08, 9.2384e-07,  ..., 5.4144e-07,\n",
      "           9.7626e-06, 7.0886e-04],\n",
      "          [5.3602e-05, 1.6596e-06, 1.0307e-04,  ..., 2.7345e-05,\n",
      "           5.0346e-04, 8.8534e-04]]]])\n"
     ]
    }
   ],
   "source": [
    "print(sample.view(64, 1, 28, 28))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "books",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
